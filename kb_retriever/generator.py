"""
Generator module for RAG system using Allam7B-Physiology-RAG-finetuned-final model.
"""

from __future__ import annotations

import logging
import os
from pathlib import Path
from typing import Dict, List, Optional

import torch
from peft import PeftModel
from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def detect_device() -> str:
    """Auto-detect best available device (GPU > CPU)."""
    if torch.cuda.is_available():
        device_name = torch.cuda.get_device_name(0)
        memory_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        logger.info(f"[DEVICE] GPU detected: {device_name} ({memory_gb:.1f} GB)")
        return "cuda"
    else:
        logger.info("[DEVICE] No GPU detected, using CPU")
        return "cpu"


class RAGGenerator:
    """Generator for RAG system using fine-tuned Allam7B model."""
    
    def __init__(
        self,
        model_path: str,
        base_model_name: str = "humain-ai/ALLaM-7B-Instruct-preview",
        device: str = "cpu",
        load_in_4bit: bool = False,
        load_in_8bit: bool = False,
        torch_dtype: Optional[torch.dtype] = None,
    ):
        """
        Initialize the RAG generator.
        
        Args:
            model_path: Path to the fine-tuned adapter model directory
            base_model_name: Name of the base model on HuggingFace
            device: Device to run on ('cpu', 'cuda', etc.)
            load_in_4bit: Whether to load in 4-bit quantization
            load_in_8bit: Whether to load in 8-bit quantization
            torch_dtype: Optional torch dtype (e.g., torch.float16)
        """
        self.model_path = Path(model_path)
        self.base_model_name = base_model_name
        self.device = device
        self.load_in_4bit = load_in_4bit
        self.load_in_8bit = load_in_8bit
        self.torch_dtype = torch_dtype or torch.float32
        
        self.model = None
        self.tokenizer = None
        self._initialized = False
        
        logger.info(f"[GENERATOR] Initializing with model_path={model_path}, device={device}")
    
    def initialize(self):
        """Load the model and tokenizer."""
        if self._initialized:
            logger.info("[GENERATOR] Already initialized, skipping...")
            return
        
        try:
            # Load tokenizer with fallback to base model and slow tokenizer
            logger.info(f"[GENERATOR] Loading tokenizer from {self.model_path}...")
            try:
                # Try fast tokenizer first
                self.tokenizer = AutoTokenizer.from_pretrained(
                    str(self.model_path),
                    trust_remote_code=True,
                    use_fast=True
                )
                logger.info("[GENERATOR] âœ… Tokenizer loaded from model path (fast)")
            except Exception as e:
                logger.warning(f"[GENERATOR] âš ï¸ Fast tokenizer failed: {e}")
                try:
                    # Try slow tokenizer from model path
                    logger.info("[GENERATOR] Trying slow tokenizer from model path...")
                    self.tokenizer = AutoTokenizer.from_pretrained(
                        str(self.model_path),
                        trust_remote_code=True,
                        use_fast=False
                    )
                    logger.info("[GENERATOR] âœ… Tokenizer loaded from model path (slow)")
                except Exception as e2:
                    logger.warning(f"[GENERATOR] âš ï¸ Model path tokenizer failed: {e2}")
                    logger.info(f"[GENERATOR] Falling back to base model tokenizer: {self.base_model_name}")
                    try:
                        # Try fast tokenizer from base model
                        self.tokenizer = AutoTokenizer.from_pretrained(
                            self.base_model_name,
                            trust_remote_code=True,
                            use_fast=True
                        )
                        logger.info("[GENERATOR] âœ… Tokenizer loaded from base model (fast)")
                    except Exception as e3:
                        logger.warning(f"[GENERATOR] âš ï¸ Base model fast tokenizer failed: {e3}")
                        # Final fallback: slow tokenizer from base model
                        logger.info("[GENERATOR] Using slow tokenizer from base model...")
                        self.tokenizer = AutoTokenizer.from_pretrained(
                            self.base_model_name,
                            trust_remote_code=True,
                            use_fast=False
                        )
                        logger.info("[GENERATOR] âœ… Tokenizer loaded from base model (slow)")
            
            # Set pad token if not set
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            logger.info(f"[GENERATOR] Loading base model: {self.base_model_name}...")
            
            # Prepare model loading kwargs
            model_kwargs = {
                "trust_remote_code": True,
            }
            
            # Configure quantization based on device
            if self.device == "cuda":
                # GPU: bitsandbytes 4-bit is FAST and memory-efficient
                if self.load_in_4bit:
                    try:
                        from transformers import BitsAndBytesConfig
                        # Test if bitsandbytes is properly installed
                        import bitsandbytes as bnb
                        model_kwargs["low_cpu_mem_usage"] = True
                        model_kwargs["quantization_config"] = BitsAndBytesConfig(
                            load_in_4bit=True,
                            bnb_4bit_compute_dtype=torch.float16,
                            bnb_4bit_use_double_quant=True,
                            bnb_4bit_quant_type="nf4"
                        )
                        logger.info("[GENERATOR] âœ… Using 4-bit quantization on GPU (fast & efficient)")
                    except (ImportError, AttributeError, ModuleNotFoundError, RuntimeError) as e:
                        logger.warning(f"[GENERATOR] âš ï¸ bitsandbytes not available: {e}")
                        logger.warning("[GENERATOR] Falling back to float16 (still fast on GPU)")
                        model_kwargs["torch_dtype"] = torch.float16
                        model_kwargs["low_cpu_mem_usage"] = True
                        self.load_in_4bit = False  # Disable 4-bit for this session
                elif self.load_in_8bit:
                    model_kwargs["load_in_8bit"] = True
                    model_kwargs["low_cpu_mem_usage"] = True
                    logger.info("[GENERATOR] Using 8-bit quantization on GPU")
                else:
                    # Full precision on GPU (faster but uses more memory)
                    model_kwargs["torch_dtype"] = torch.float16
                    model_kwargs["low_cpu_mem_usage"] = True
                    logger.info("[GENERATOR] Using float16 on GPU (full precision)")
            else:
                # CPU: bitsandbytes is EXTREMELY slow (0.1 tokens/sec)
                # Must use regular dtypes instead
                if self.load_in_4bit or self.load_in_8bit:
                    logger.warning("[GENERATOR] âš ï¸ bitsandbytes disabled on CPU (extremely slow)")
                    logger.warning("[GENERATOR] Using float16 instead for CPU performance")
                # Use float16 for CPU - much faster than bitsandbytes 4-bit
                model_kwargs["torch_dtype"] = torch.float16
                model_kwargs["low_cpu_mem_usage"] = True
                logger.info("[GENERATOR] Using float16 on CPU")
            
            # Set device_map only for CUDA
            if self.device == "cuda":
                model_kwargs["device_map"] = "auto"
            else:
                # For CPU, don't use device_map (loads directly to CPU)
                model_kwargs["device_map"] = None
            
            # Load base model
            logger.info("[GENERATOR] Loading base model (this may take a few minutes)...")
            self.model = AutoModelForCausalLM.from_pretrained(
                self.base_model_name,
                **model_kwargs
            )
            
            # Move to device if not using device_map
            if self.device != "cuda" or model_kwargs.get("device_map") is None:
                self.model = self.model.to(self.device)
            
            logger.info(f"[GENERATOR] Loading LoRA adapter from {self.model_path}...")
            # Prepare adapter loading kwargs
            adapter_kwargs = {}
            if self.device == "cuda":
                adapter_kwargs["device_map"] = "auto"
            # For CPU, don't pass device_map - let it use the model's current device
            
            self.model = PeftModel.from_pretrained(
                self.model,
                str(self.model_path),
                **adapter_kwargs
            )
            
            # Ensure model is on correct device after loading adapter
            if self.device != "cuda":
                self.model = self.model.to(self.device)
            
            # Merge adapter if needed (for faster inference)
            # self.model = self.model.merge_and_unload()
            
            self.model.eval()
            self._initialized = True
            
            logger.info("[GENERATOR] âœ… Model loaded successfully!")
            
        except Exception as e:
            logger.error(f"[GENERATOR] âŒ Error loading model: {e}")
            raise
    
    def generate(
        self,
        query: str,
        context: Optional[str] = None,
        chunks: Optional[List[Dict]] = None,
        max_new_tokens: int = 512,
        temperature: float = 0.7,
        top_p: float = 0.9,
        top_k: int = 50,
        do_sample: bool = True,
        return_full_text: bool = False,
    ) -> str:
        """
        Generate response to query, optionally with context from KB.
        
        Args:
            query: User query
            context: Optional pre-formatted context string (if provided, chunks are ignored)
            chunks: Optional list of retrieved chunks (will be summarized if provided)
            max_new_tokens: Maximum tokens to generate
            temperature: Sampling temperature
            top_p: Nucleus sampling parameter
            top_k: Top-k sampling parameter
            do_sample: Whether to use sampling
            return_full_text: Whether to return full prompt + response
        
        Returns:
            Generated response text
        """
        if not self._initialized:
            raise RuntimeError("Generator not initialized. Call initialize() first.")
        
        # Summarize chunks if provided (preferred over raw context)
        if chunks and not context:
            logger.info(f"[GENERATOR] Summarizing {len(chunks)} retrieved chunks...")
            context = self.summarize_context(chunks)
            logger.info(f"[GENERATOR] Created clean summary ({len(context)} chars)")
        
        # Build prompt
        if context:
            # RAG prompt with context
            prompt = self._build_rag_prompt(query, context)
            logger.info(f"[GENERATOR] Using RAG mode with context ({len(context)} chars)")
        else:
            # General knowledge mode
            prompt = self._build_general_prompt(query)
            logger.info("[GENERATOR] Using general knowledge mode (no context)")
        
        # Tokenize
        inputs = self.tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=2048  # Reasonable limit
        ).to(self.device)
        
        # Optimize generation settings based on device
        import time
        start_time = time.time()
        input_length = inputs['input_ids'].shape[1]
        
        # Limit max_new_tokens for shorter responses (300-500 chars â‰ˆ 75-125 tokens)
        # Use conservative limits to ensure concise answers
        target_max_tokens = min(max_new_tokens, 150)  # Cap at 150 tokens for concise answers
        
        if self.device == "cuda":
            # GPU-optimized settings: faster, can handle more tokens
            expected_time = "10-30 seconds"
            max_tokens = target_max_tokens
            top_k_value = top_k  # No reduction on GPU
            logger.info(f"[GENERATOR] ðŸš€ GPU mode: Starting generation (max_new_tokens={max_tokens}, input_tokens={input_length})...")
            logger.info(f"[GENERATOR] Expected time: {expected_time}")
        else:
            # CPU-optimized settings: conservative to prevent hanging
            expected_time = "2-5 minutes"
            max_tokens = min(target_max_tokens, 100)  # Cap at 100 for CPU
            top_k_value = min(top_k, 40)  # Reduce top_k for faster CPU generation
            logger.info(f"[GENERATOR] ðŸ’» CPU mode: Starting generation (max_new_tokens={max_tokens}, input_tokens={input_length})...")
            logger.info(f"[GENERATOR] Expected time: {expected_time}")
        
        # Log device info
        # Get actual model device (may be different if offloaded by accelerate)
        model_device = next(self.model.parameters()).device
        logger.info(f"[GENERATOR] Model device: {model_device}")
        logger.info(f"[GENERATOR] Input device: {inputs['input_ids'].device}")
        
        # Move inputs to match model device (important for accelerate offloading)
        if inputs['input_ids'].device != model_device:
            logger.info(f"[GENERATOR] Moving inputs from {inputs['input_ids'].device} to {model_device}")
            inputs = {k: v.to(model_device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}
        
        try:
            with torch.inference_mode():  # Memory efficient
                logger.info(f"[GENERATOR] Calling model.generate()...")
                
                # Build generation kwargs
                generation_kwargs = {
                    **inputs,
                    "max_new_tokens": max_tokens,
                    "temperature": temperature,
                    "top_p": top_p,
                    "top_k": top_k_value,
                    "do_sample": do_sample,
                    "pad_token_id": self.tokenizer.pad_token_id,
                    "eos_token_id": self.tokenizer.eos_token_id,
                    "repetition_penalty": 1.1,
                    "use_cache": True,  # KV cache for efficiency
                }
                
                # GPU-specific optimizations
                if self.device == "cuda":
                    generation_kwargs["num_beams"] = 1  # Greedy decoding (fast on GPU)
                    # Note: Flash Attention is configured at model load time, not during generation
                    # The model will use it automatically if available
                else:
                    generation_kwargs["num_beams"] = 1  # Greedy decoding (faster on CPU)
                
                outputs = self.model.generate(**generation_kwargs)
                
                logger.info(f"[GENERATOR] model.generate() completed, output shape: {outputs.shape}")
                
        except Exception as e:
            elapsed = time.time() - start_time
            logger.error(f"[GENERATOR] âŒ Generation failed after {elapsed:.1f}s: {e}")
            import traceback
            logger.error(f"[GENERATOR] Traceback: {traceback.format_exc()}")
            raise
        
        generation_time = time.time() - start_time
        logger.info(f"[GENERATOR] âœ… Generation completed in {generation_time:.1f}s ({generation_time/60:.1f} minutes)")
        
        # Decode
        generated_text = self.tokenizer.decode(
            outputs[0],
            skip_special_tokens=True
        )
        
        if return_full_text:
            cleaned_text = generated_text
        else:
            # Extract only the generated part (after the prompt)
            prompt_length = len(self.tokenizer.decode(inputs["input_ids"][0], skip_special_tokens=True))
            response = generated_text[prompt_length:].strip()
            cleaned_text = response
        
        # Post-process: remove names, dates, artifact tokens, credentials
        cleaned_text = self._clean_response(cleaned_text)
        
        # Enforce length (target 200-400 chars; hard cap 400)
        if len(cleaned_text) > 400:
            sentences = cleaned_text.split('.')
            truncated = ""
            for s in sentences:
                if len(truncated + s + '.') <= 400:
                    truncated += s + '.'
                else:
                    break
            cleaned_text = truncated.strip() if truncated else cleaned_text[:400].strip()
        
        # Ensure recommendation if missing and there is room
        if 'Ù…Ø®ØªØµ' not in cleaned_text and 'Ø·Ø¨ÙŠØ¨' not in cleaned_text and 'Ø§Ø³ØªØ´Ø§Ø±Ø©' not in cleaned_text:
            if len(cleaned_text) < 350:
                cleaned_text += " ÙŠÙÙ†ØµØ­ Ø¨Ù…Ø±Ø§Ø¬Ø¹Ø© Ø·Ø¨ÙŠØ¨ Ù†ÙØ³ÙŠ Ù…Ø®ØªØµ."
        
        return cleaned_text
    
    def summarize_context(self, chunks: List[Dict]) -> str:
        """
        Summarize and clean retrieved chunks into a single coherent context.
        Removes names, greetings, closing phrases, and merges into clean summary.
        """
        import re
        
        # Extract text from chunks
        texts = []
        for chunk in chunks:
            text = chunk.get("text", chunk.get("clean_text", ""))
            if text:
                texts.append(text)
        
        if not texts:
            return ""
        
        # Clean each text - using same patterns as test script
        cleaned_texts = []
        for text in texts:
            # Remove common greetings (same as test script)
            text = re.sub(r'Ø¨Ø³Ù… Ø§Ù„Ù„Ù‡[^.]*\.', '', text)
            text = re.sub(r'Ø§Ù„Ø£Ø®Øª Ø§Ù„ÙØ§Ø¶Ù„Ø©/[^ØŒ\n]+', '', text)
            text = re.sub(r'Ø§Ù„Ø£Ø® Ø§Ù„ÙØ§Ø¶Ù„/[^ØŒ\n]+', '', text)
            text = re.sub(r'Ø­ÙØ¸Ù‡Ø§ Ø§Ù„Ù„Ù‡|Ø­ÙØ¸Ù‡ Ø§Ù„Ù„Ù‡', '', text)
            text = re.sub(r'Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ…[^.]*\.', '', text)
            
            # Remove closing phrases (same as test script)
            text = re.sub(r'Ø§Ù†ØªÙ‡Øª Ø¥Ø¬Ø§Ø¨Ø©[^.]*\.', '', text)
            text = re.sub(r'ØªÙ„ÙŠÙ‡Ø§ Ø¥Ø¬Ø§Ø¨Ø©[^.]*\.', '', text)
            text = re.sub(r'ÙˆØªØ¶ÙŠÙ[^.]*:', '', text)
            text = re.sub(r'Ø¨Ø§Ø±Ùƒ Ø§Ù„Ù„Ù‡ ÙÙŠÙƒ[^.]*\.', '', text)
            text = re.sub(r'Ø¬Ø²Ø§Ùƒ Ø§Ù„Ù„Ù‡ Ø®ÙŠØ±Ù‹Ø§[^.]*\.', '', text)
            text = re.sub(r'ÙˆØ¨Ø§Ù„Ù„Ù‡ Ø§Ù„ØªÙˆÙÙŠÙ‚[^.]*\.', '', text)
            text = re.sub(r'ÙˆÙÙ‚Ùƒ Ø§Ù„Ù„Ù‡[^.]*\.', '', text)
            text = re.sub(r'ÙˆÙÙ‚ÙƒÙ… Ø§Ù„Ù„Ù‡[^.]*\.', '', text)
            text = re.sub(r'ÙˆØ§Ù„Ù„Ù‡ Ø§Ù„Ù…ÙˆÙÙ‚[^.]*\.', '', text)
            text = re.sub(r'Ø´ÙƒØ±Ø§Ù‹ Ù„Ø³Ø¤Ø§Ù„Ùƒ[^.]*\.', '', text)
            text = re.sub(r'Ø´ÙƒØ±Ø§ Ù„Ø³Ø¤Ø§Ù„Ùƒ[^.]*\.', '', text)
            text = re.sub(r'Ø´ÙƒØ±Ø§Ù‹ Ù„Ùƒ[^.]*\.', '', text)
            text = re.sub(r'Ø´ÙƒØ±Ø§ Ù„Ùƒ[^.]*\.', '', text)
            
            # Remove specific names (same as test script)
            text = re.sub(r'Ø§Ù„Ø¯ÙƒØªÙˆØ±[^ØŒ\n]+(?:Ø§Ø³ØªØ´Ø§Ø±ÙŠ|Ù…Ø³ØªØ´Ø§Ø±)[^ØŒ\n]*', '', text)
            text = re.sub(r'Ø§Ù„Ù…Ø³ØªØ´Ø§Ø±[^ØŒ\n]+', '', text)
            text = re.sub(r'Ø§Ù„Ù…Ø³ØªØ´Ø§Ø±Ø©[^ØŒ\n]+', '', text)
            
            # Remove doctor/person names (single or multiple words)
            text = re.sub(r'Ø¯\.\s+[Ø£-ÙŠ]+(?:\s+[Ø£-ÙŠ]+)*', '', text)
            text = re.sub(r'Ø¯ÙƒØªÙˆØ±Ø©\s+[Ø£-ÙŠ]+(?:\s+[Ø£-ÙŠ]+)*', '', text)
            text = re.sub(r'Ø§Ù„Ø¯ÙƒØªÙˆØ±\s+[Ø£-ÙŠ]+(?:\s+[Ø£-ÙŠ]+)*', '', text)
            
            # Remove tags and artifact tokens
            text = re.sub(r'\[/INST\]', '', text)
            text = re.sub(r'INSTAINSTANT_ANSWER', '', text)
            text = re.sub(r'INSTANT_ANSWER', '', text)
            text = re.sub(r'INST[A-Za-z0-9_]*', '', text)
            text = re.sub(r'[A-Z]{2,}_[A-Za-z0-9_]+', '', text)
            
            # Remove dates and metadata stamps
            text = re.sub(r'ØªÙ… Ø§Ù„ØªØ­Ø¯ÙŠØ« Ø¨ØªØ§Ø±ÙŠØ®\s*[\dÙ -Ù©/]+', '', text)
            text = re.sub(r'Ø·Ø¨ÙŠØ¨Ø©\s+Ø¹Ø§Ù…Ø©\s*\.?', '', text)
            text = re.sub(r'Ø·Ø¨ÙŠØ¨\s+Ø¹Ø§Ù…\s*\.?', '', text)
            
            # Clean whitespace
            text = re.sub(r'\s+', ' ', text)
            text = re.sub(r'\n{3,}', '\n\n', text)
            
            # Remove very short fragments (likely artifacts)
            if len(text.strip()) > 50:  # Keep only substantial text
                cleaned_texts.append(text.strip())
        
        # Merge into single coherent summary
        # Remove duplicates and filter for medical content only
        medical_sentences = []
        seen = set()
        
        for text in cleaned_texts:
            # Split into sentences
            sentences = re.split(r'[.!?]\s+', text)
            for sentence in sentences:
                sentence = sentence.strip()
                if len(sentence) < 20:  # Skip very short sentences
                    continue
                
                # Simple deduplication (normalize)
                normalized = re.sub(r'[^\w\s]', '', sentence.lower())
                if normalized in seen or len(normalized) < 10:
                    continue
                
                # Only keep sentences with medical/psychological content
                medical_keywords = [
                    'Ø§ÙƒØªØ¦Ø§Ø¨', 'Ù‚Ù„Ù‚', 'Ù†ÙØ³ÙŠ', 'Ø¹Ø§Ø·ÙÙŠ', 'Ù†ÙˆÙ…', 'Ø£Ø±Ù‚', 'Ø£Ø¹Ø±Ø§Ø¶', 
                    'Ø¹Ù„Ø§Ø¬', 'Ø·Ø¨ÙŠØ¨', 'Ù…Ø®ØªØµ', 'ØµØ­Ø©', 'ØµØ­ÙŠ', 'Ù…Ø±Ø¶', 'Ø­Ø§Ù„Ø©',
                    'depression', 'anxiety', 'mental', 'health', 'symptom', 'treatment'
                ]
                sentence_lower = sentence.lower()
                if any(keyword in sentence_lower for keyword in medical_keywords):
                    medical_sentences.append(sentence)
                    seen.add(normalized)
        
        if not medical_sentences:
            return ""
        
        # Remove contradictory information (physical vs mental health)
        # If we have mental health recommendations, prioritize them and remove conflicting physical mentions
        has_mental_health = any('Ù†ÙØ³ÙŠ' in s or 'Ø¹Ø§Ø·ÙÙŠ' in s or 'Ø§ÙƒØªØ¦Ø§Ø¨' in s or 'Ù‚Ù„Ù‚' in s 
                                for s in medical_sentences)
        
        if has_mental_health:
            # Filter out sentences about physical conditions that contradict mental health focus
            filtered_sentences = []
            for sentence in medical_sentences:
                # Skip sentences that mention physical conditions when we're focusing on mental health
                physical_keywords = ['Ø¹Ø¶ÙˆÙŠ', 'Ø¬Ø³Ø¯ÙŠ', 'ÙØ­Øµ Ù…Ø®Ø¨Ø±ÙŠ', 'ØªØ­Ù„ÙŠÙ„', 'Ø·Ø¨ÙŠØ¨ Ø¹Ø§Ù…']
                if any(keyword in sentence.lower() for keyword in physical_keywords):
                    # But keep general health advice (like sleep hygiene)
                    if 'Ù†ÙˆÙ…' in sentence.lower() or 'ØµØ­Ø©' in sentence.lower():
                        filtered_sentences.append(sentence)
                else:
                    filtered_sentences.append(sentence)
            medical_sentences = filtered_sentences if filtered_sentences else medical_sentences
        
        # Create a coherent paragraph by joining sentences
        # Ensure it flows naturally as a single paragraph
        summary = ' '.join(medical_sentences)
        
        # Clean up spacing to ensure it's a single flowing paragraph
        summary = re.sub(r'\s+', ' ', summary)
        summary = re.sub(r'\n+', ' ', summary)
        
        # Limit length to prevent excessively long prompts
        if len(summary) > 1200:
            # Truncate at sentence boundary
            sentences = summary.split('.')
            summary = ""
            for sentence in sentences:
                if len(summary + sentence + '.') <= 1200:
                    summary += sentence + '.'
                else:
                    break
            if not summary:
                summary = summary[:1200] + "..."
        
        return summary.strip()
    
    def _clean_response(self, text: str) -> str:
        """Remove common patterns copied from knowledge base: names, dates, artifact tokens, credentials."""
        import re
        
        # --- Artifact tokens (placeholders, internal tags) ---
        text = re.sub(r'INST[A-Za-z0-9_]*', '', text)
        text = re.sub(r'[A-Z]{2,}[_][A-Za-z0-9_]+', '', text)  # ALL_CAPS_WITH_UNDERSCORES
        
        # --- Dates and update stamps (leaked from KB metadata) ---
        text = re.sub(r'ØªÙ… Ø§Ù„ØªØ­Ø¯ÙŠØ« Ø¨ØªØ§Ø±ÙŠØ®\s*[\dÙ -Ù©/]+', '', text)
        text = re.sub(r'ØªØ§Ø±ÙŠØ®\s*[\dÙ -Ù©/]+\s*\.?', '', text)
        
        # --- Credentials / role labels (Ø·Ø¨ÙŠØ¨Ø© Ø¹Ø§Ù…Ø©ØŒ Ø§Ø³ØªØ´Ø§Ø±ÙŠØŒ Ø¥Ù„Ø®) ---
        text = re.sub(r'Ø·Ø¨ÙŠØ¨Ø©\s+Ø¹Ø§Ù…Ø©\s*\.?', '', text)
        text = re.sub(r'Ø·Ø¨ÙŠØ¨\s+Ø¹Ø§Ù…\s*\.?', '', text)
        text = re.sub(r'Ø§Ø³ØªØ´Ø§Ø±ÙŠ[Ø©]?\s*(?:Ù†ÙØ³ÙŠ[Ø©]?|Ø·Ø¨ Ù†ÙØ³[ÙŠØ©]?)?\s*\.?', '', text)
        
        # --- Person/doctor names: Ø¯. Ø¥ÙŠÙ†Ø§Ø³ØŒ Ø¯ÙƒØªÙˆØ±Ø© XØŒ Ø§Ù„Ø¯ÙƒØªÙˆØ± X Y ---
        text = re.sub(r'Ø¯\.\s+[Ø£-ÙŠ]+\s*(?:[Ø£-ÙŠ]+\s*)*', '', text)
        text = re.sub(r'Ø¯ÙƒØªÙˆØ±Ø©\s+[Ø£-ÙŠ]+\s*(?:[Ø£-ÙŠ]+\s*)*', '', text)
        text = re.sub(r'Ø§Ù„Ø¯ÙƒØªÙˆØ±\s+[Ø£-ÙŠ]+\s*(?:[Ø£-ÙŠ]+\s*)*', '', text)
        text = re.sub(r'Ø§Ù„Ø¯ÙƒØªÙˆØ±[^ØŒ.\n]+(?:Ø§Ø³ØªØ´Ø§Ø±ÙŠ|Ù…Ø³ØªØ´Ø§Ø±)[^ØŒ.\n]*', '', text)
        text = re.sub(r'Ø§Ù„Ù…Ø³ØªØ´Ø§Ø±[Ø©]?[^ØŒ.\n]+', '', text)
        
        # --- Common greetings with names ---
        text = re.sub(r'Ø¨Ø³Ù… Ø§Ù„Ù„Ù‡ Ø§Ù„Ø±Ø­Ù…Ù† Ø§Ù„Ø±Ø­ÙŠÙ…\s*\n*', '', text)
        text = re.sub(r'Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ…[^.]*\.', '', text)
        text = re.sub(r'Ø§Ù„Ø£Ø®Øª Ø§Ù„ÙØ§Ø¶Ù„Ø©/[^ØŒ\n]+', '', text)
        text = re.sub(r'Ø§Ù„Ø£Ø® Ø§Ù„ÙØ§Ø¶Ù„/[^ØŒ\n]+', '', text)
        text = re.sub(r'Ø§Ù„Ø§Ø¨Ù†Ø© Ø§Ù„ÙØ§Ø¶Ù„Ø©/[^ØŒ\n]+', '', text)
        text = re.sub(r'Ø­ÙØ¸Ù‡Ø§ Ø§Ù„Ù„Ù‡|Ø­ÙØ¸Ù‡ Ø§Ù„Ù„Ù‡', '', text)
        
        # --- Closing phrases and thank you messages ---
        text = re.sub(r'Ø§Ù†ØªÙ‡Øª Ø¥Ø¬Ø§Ø¨Ø©[^.]*\.', '', text)
        text = re.sub(r'ØªÙ„ÙŠÙ‡Ø§ Ø¥Ø¬Ø§Ø¨Ø©[^.]*\.', '', text)
        text = re.sub(r'ÙˆØªØ¶ÙŠÙ[^.]*:', '', text)
        text = re.sub(r'Ø¨Ø§Ø±Ùƒ Ø§Ù„Ù„Ù‡ ÙÙŠÙƒ[^.]*\.', '', text)
        text = re.sub(r'Ø¬Ø²Ø§Ùƒ Ø§Ù„Ù„Ù‡ Ø®ÙŠØ±Ù‹Ø§[^.]*\.', '', text)
        text = re.sub(r'ÙˆØ¨Ø§Ù„Ù„Ù‡ Ø§Ù„ØªÙˆÙÙŠÙ‚[^.]*\.', '', text)
        text = re.sub(r'ÙˆÙÙ‚Ùƒ Ø§Ù„Ù„Ù‡[^.]*\.', '', text)
        text = re.sub(r'ÙˆÙÙ‚ÙƒÙ… Ø§Ù„Ù„Ù‡[^.]*\.', '', text)
        text = re.sub(r'ÙˆØ§Ù„Ù„Ù‡ Ø§Ù„Ù…ÙˆÙÙ‚[^.]*\.', '', text)
        text = re.sub(r'Ø´ÙƒØ±Ø§Ù‹ Ù„Ø³Ø¤Ø§Ù„Ùƒ[^.]*\.', '', text)
        text = re.sub(r'Ø´ÙƒØ±Ø§ Ù„Ø³Ø¤Ø§Ù„Ùƒ[^.]*\.', '', text)
        text = re.sub(r'Ø´ÙƒØ±Ø§Ù‹ Ù„Ùƒ[^.]*\.', '', text)
        text = re.sub(r'Ø´ÙƒØ±Ø§ Ù„Ùƒ[^.]*\.', '', text)
        
        # --- Greetings ---
        text = re.sub(r'Ù…Ø±Ø­Ø¨Ø§Ù‹[ØŒ,]?\s*', '', text)
        
        # --- Tags ---
        text = re.sub(r'\[/INST\]', '', text)
        text = re.sub(r'INSTAINSTANT_ANSWER', '', text)
        text = re.sub(r'INSTANT_ANSWER', '', text)
        
        # --- Leading stray period or fragment ---
        text = re.sub(r'^\s*[.,ØŒ]\s*', '', text)
        
        # --- Cleanup ---
        text = re.sub(r'\n{3,}', '\n\n', text)
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'[ØŒ,]\s*$', '', text)
        text = text.strip()
        
        # --- Trim obviously incomplete tail (e.g. "Ø¥Ù† Ù…Ø§ Ø­Ø¯Ø«" with no ending) ---
        if len(text) > 60 and text[-1] not in '.?!ã€‚':
            last_period = text.rfind('.')
            if last_period > 0:
                tail = text[last_period + 1:].strip()
                if 0 < len(tail) < 25:  # short fragment, likely incomplete
                    text = text[:last_period + 1].strip()
        
        return text
    
    def _build_rag_prompt(self, query: str, context: str) -> str:
        """Build RAG prompt with context."""
        
        # Check for critical safety topics
        critical_keywords = ['Ø§Ù†ØªØ­Ø§Ø±', 'Ø§Ù†ØªØ­Ø§Ø±ÙŠ', 'suicide', 'Ù‚ØªÙ„ Ù†ÙØ³', 'Ø¥Ù†Ù‡Ø§Ø¡ Ø§Ù„Ø­ÙŠØ§Ø©', 'Ø£Ø±ÙŠØ¯ Ø§Ù„Ù…ÙˆØª', 'Ø£ÙØ¶Ù„ Ø§Ù„Ù…ÙˆØª', 'Ù…Ù…Ù„Ø©', 'Ù„Ø§ ÙØ§Ø¦Ø¯Ø©', 'Ù„Ø§ Ù…Ø¹Ù†Ù‰', 'Ù„Ø§ Ø£Ù…Ù„']
        is_critical = any(keyword in query.lower() for keyword in critical_keywords)
        
        system_message = """Ø£Ù†Øª Ø®Ø¨ÙŠØ± ÙØ³ÙŠÙˆÙ„ÙˆØ¬ÙŠ ÙˆÙ†ÙØ³ÙŠ Ù…ØªØ®ØµØµ. Ù‚Ø¯Ù… Ø¥Ø¬Ø§Ø¨Ø§Øª Ø¯Ø§Ø¹Ù…Ø© ÙˆÙ…Ø·Ù…Ø¦Ù†Ø© Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙÙ‚Ø·.

ØªØ¹Ù„ÙŠÙ…Ø§Øª ØµØ§Ø±Ù…Ø©:
1. Ø§Ø¨Ø¯Ø£ Ù…Ø¨Ø§Ø´Ø±Ø© Ø¨Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© - Ù„Ø§ ØªØ­ÙŠØ§Øª ÙˆÙ„Ø§ Ø£Ø³Ù…Ø§Ø¡ (Ù„Ø§ "Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ…"ØŒ "Ø¨Ø³Ù… Ø§Ù„Ù„Ù‡"ØŒ Ø£Ø³Ù…Ø§Ø¡ Ø£Ø·Ø¨Ø§Ø¡ Ø£Ùˆ Ø£Ø´Ø®Ø§Øµ).
2. Ø§Ø³ØªØ®Ø¯Ù… ÙÙ‚Ø· Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø·Ø¨ÙŠØ© Ù…Ù† Ø§Ù„Ø³ÙŠØ§Ù‚ - Ù„Ø§ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©. Ù„Ø§ ØªÙ†Ø³Ø® Ù…Ù† Ø§Ù„Ù†Øµ: ØªÙˆØ§Ø±ÙŠØ®ØŒ ØªÙˆÙ‚ÙŠØ¹Ø§ØªØŒ "ØªÙ… Ø§Ù„ØªØ­Ø¯ÙŠØ« Ø¨ØªØ§Ø±ÙŠØ®"ØŒ "Ø·Ø¨ÙŠØ¨Ø© Ø¹Ø§Ù…Ø©"ØŒ Ø£Ùˆ Ø±Ù…ÙˆØ² (Ù…Ø«Ù„ INST_).
3. Ù„Ø§ ØªØ°ÙƒØ± Ø£Ø³Ù…Ø§Ø¡ Ø£Ø¯ÙˆÙŠØ© ÙˆÙ„Ø§ Ø¬Ø±Ø¹Ø§Øª ÙˆÙ„Ø§ ØªÙØ§ØµÙŠÙ„ Ø¯ÙˆØ§Ø¦ÙŠØ© - Ø§ÙƒØªÙÙ Ø¯Ø§Ø¦Ù…Ø§Ù‹ Ø¨ØªÙˆØµÙŠØ© Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„Ø·Ø¨ÙŠØ¨ Ø£Ùˆ Ø§Ù„ØµÙŠØ¯Ù„ÙŠ.
4. Ù‚Ø¯Ù… Ø±Ø³Ø§Ù„Ø© Ù…Ø·Ù…Ø¦Ù†Ø© ÙˆØ¯Ø§Ø¹Ù…Ø© Ø«Ù… ØªÙˆØµÙŠØ© ÙˆØ§Ø¶Ø­Ø© Ù„Ø§Ø³ØªØ´Ø§Ø±Ø© Ø§Ù„Ù…Ø®ØªØµÙŠÙ†.
5. ØªØ¬Ù†Ø¨ Ø§Ù„ØªÙ†Ø§Ù‚Ø¶Ø§Øª - Ù„Ø§ Ø­Ø§Ù„Ø§Øª Ø¹Ø¶ÙˆÙŠØ© Ø¥Ø°Ø§ Ø§Ù„Ø­Ø§Ù„Ø© Ù†ÙØ³ÙŠØ© ÙˆØ§Ù„Ø¹ÙƒØ³.
6. Ø§Ù„Ø·ÙˆÙ„: 200â€“400 Ø­Ø±Ù ÙÙ‚Ø· (Ù…Ø®ØªØµØ± ÙƒØ¥Ø¬Ø§Ø¨Ø© Ø£Ø®ØµØ§Ø¦ÙŠØŒ 2â€“4 Ø¬Ù…Ù„). Ù„Ø§ ØªØ·ÙŠÙ„.
7. Ø£Ù†Ù‡Ù Ø§Ù„Ø¬Ù…Ù„Ø© Ø­ØªÙ‰ Ø§Ù„Ù†Ù‡Ø§ÙŠØ© Ø«Ù… ØªÙˆÙ‚Ù - Ù„Ø§ ØªØªØ±Ùƒ Ø¬Ù…Ù„Ø© Ù†Ø§Ù‚ØµØ©.
8. Ø±ÙƒØ² Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø·Ø¨ÙŠØ© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© ÙÙ‚Ø· - Ù„Ø§ Ø£Ø³Ù…Ø§Ø¡ØŒ Ù„Ø§ ØªÙˆØ§Ø±ÙŠØ®ØŒ Ù„Ø§ Ø£Ø¯ÙˆÙŠØ© Ø¨Ø£Ø³Ù…Ø§Ø¦Ù‡Ø§.

Ù‡ÙŠÙƒÙ„ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:
1. Ø±Ø³Ø§Ù„Ø© Ù…Ø·Ù…Ø¦Ù†Ø©: Ø§Ø¨Ø¯Ø£ Ø¨ØªØ·Ù…ÙŠÙ† Ø§Ù„Ø³Ø§Ø¦Ù„ Ø£Ù† Ù…Ø§ ÙŠÙ…Ø± Ø¨Ù‡ Ù…ÙÙ‡ÙˆÙ… ÙˆÙŠÙ…ÙƒÙ† Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹Ù‡
2. Ø´Ø±Ø­ Ù…Ø®ØªØµØ±: Ø§Ø´Ø±Ø­ Ø§Ù„Ø­Ø§Ù„Ø© Ø¨Ø´ÙƒÙ„ Ù…Ø¨Ø³Ø· ÙˆÙ…Ø·Ù…Ø¦Ù† (2-3 Ø¬Ù…Ù„)
3. ØªÙˆØµÙŠØ© Ø¯Ø§Ø¹Ù…Ø©: Ø´Ø¬Ø¹ Ø¹Ù„Ù‰ Ø·Ù„Ø¨ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ù‡Ù†ÙŠØ© Ø¨Ø·Ø±ÙŠÙ‚Ø© Ø¯Ø§Ø¹Ù…Ø© ÙˆØºÙŠØ± Ù…Ø®ÙŠÙØ©

Ù„Ù„Ù…ÙˆØ§Ø¶ÙŠØ¹ Ø§Ù„Ø­Ø±Ø¬Ø© (Ø§Ù†ØªØ­Ø§Ø±ØŒ Ø¥ÙŠØ°Ø§Ø¡ Ø§Ù„Ù†ÙØ³):
- Ø§Ø¨Ø¯Ø£ ÙÙˆØ±Ø§Ù‹ Ø¨ØªØ£ÙƒÙŠØ¯ Ø£Ù† Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ù…ØªØ§Ø­Ø© ÙˆØ£Ù† Ù‡Ø°Ù‡ Ø§Ù„Ø£ÙÙƒØ§Ø± ÙŠÙ…ÙƒÙ† Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹Ù‡Ø§
- Ù‚Ø¯Ù… Ø±Ø³Ø§Ù„Ø© Ø£Ù…Ù„ Ù‚ÙˆÙŠØ©
- Ø´Ø¬Ø¹ Ø¹Ù„Ù‰ Ø§Ù„ØªÙˆØ§ØµÙ„ Ø§Ù„ÙÙˆØ±ÙŠ Ù…Ø¹ Ù…Ø®ØªØµ Ù†ÙØ³ÙŠ Ø£Ùˆ Ø®Ø· Ù…Ø³Ø§Ø¹Ø¯Ø©
- Ø±ÙƒØ² Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù…Ù„ ÙˆØ¥Ù…ÙƒØ§Ù†ÙŠØ© Ø§Ù„ØªØ­Ø³Ù†"""
        
        # Safety message for critical topics
        safety_message = ""
        if is_critical:
            safety_message = """
âš ï¸ Ù…ÙˆØ¶ÙˆØ¹ Ø­Ø±Ø¬ - Ø§Ø¨Ø¯Ø£ ÙÙˆØ±Ø§Ù‹:
- Ø£ÙƒØ¯ Ø£Ù† Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© Ù…ØªØ§Ø­Ø© ÙˆÙ‡Ø°Ù‡ Ø§Ù„Ø£ÙÙƒØ§Ø± ÙŠÙ…ÙƒÙ† Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹Ù‡Ø§
- Ø´Ø¬Ø¹ Ø¹Ù„Ù‰ Ø§Ù„ØªÙˆØ§ØµÙ„ Ø§Ù„ÙÙˆØ±ÙŠ Ù…Ø¹ Ù…Ø®ØªØµ Ù†ÙØ³ÙŠ Ø£Ùˆ Ø®Ø· Ù…Ø³Ø§Ø¹Ø¯Ø©
- Ø±ÙƒØ² Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù…Ù„ ÙˆØ¥Ù…ÙƒØ§Ù†ÙŠØ© Ø§Ù„ØªØ­Ø³Ù†
- Ù„Ø§ ØªÙ‚Ù„Ù„ Ù…Ù† Ø®Ø·ÙˆØ±Ø© Ø§Ù„ÙˆØ¶Ø¹
"""
        
        user_message = f"""{safety_message}

Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø·Ø¨ÙŠØ© Ø§Ù„Ù…Ø±Ø¬Ø¹ÙŠØ© (Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…ÙˆØ­Ø¯Ø© ÙˆÙ…Ù†Ø³Ù‚Ø© - Ø§Ø³ØªØ®Ø¯Ù…Ù‡Ø§ ÙÙ‚Ø·):

{context}

Ø§Ù„Ø³Ø¤Ø§Ù„: {query}

Ø£Ø¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨Ù‡Ø°Ø§ Ø§Ù„Ù‡ÙŠÙƒÙ„:
1. Ø±Ø³Ø§Ù„Ø© Ù…Ø·Ù…Ø¦Ù†Ø© ÙˆØ¯Ø§Ø¹Ù…Ø© (Ù…Ø§ ÙŠÙ…Ø± Ø¨Ù‡ Ø§Ù„Ø³Ø§Ø¦Ù„ Ù…ÙÙ‡ÙˆÙ… ÙˆÙŠÙ…ÙƒÙ† Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹Ù‡)
2. Ø´Ø±Ø­ Ù…Ø®ØªØµØ± Ù„Ù„Ø­Ø§Ù„Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© (2â€“3 Ø¬Ù…Ù„)
3. ØªÙˆØµÙŠØ© Ù„Ø§Ø³ØªØ´Ø§Ø±Ø© Ø§Ù„Ù…Ø®ØªØµÙŠÙ†

âš ï¸ Ù…Ù‡Ù…:
- Ø§Ù„Ø·ÙˆÙ„: 200â€“400 Ø­Ø±Ù (Ù…Ø®ØªØµØ±). Ù„Ø§ ØªØ·ÙŠÙ„. Ø£Ù†Ù‡Ù Ø§Ù„ÙÙƒØ±Ø© Ø«Ù… ØªÙˆÙ‚Ù.
- Ù„Ø§ ØªØ­ÙŠØ§ØªØŒ Ù„Ø§ Ø£Ø³Ù…Ø§Ø¡ Ø£Ø´Ø®Ø§Øµ Ø£Ùˆ Ø£Ø·Ø¨Ø§Ø¡ØŒ Ù„Ø§ ØªÙˆØ§Ø±ÙŠØ®ØŒ Ù„Ø§ "ØªÙ… Ø§Ù„ØªØ­Ø¯ÙŠØ«"ØŒ Ù„Ø§ Ø£Ø¯ÙˆÙŠØ© Ø¨Ø£Ø³Ù…Ø§Ø¦Ù‡Ø§ Ø£Ùˆ Ø¬Ø±Ø¹Ø§Øª.
- Ø§Ø¨Ø¯Ø£ Ù…Ø¨Ø§Ø´Ø±Ø© Ø¨Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©. Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ù‚Ø¯Ù…Ø© ÙÙ‚Ø·."""
        
        # Use chat template
        messages = [
            {"role": "system", "content": system_message},
            {"role": "user", "content": user_message}
        ]
        
        prompt = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        return prompt
    
    def _build_general_prompt(self, query: str) -> str:
        """Build prompt for general knowledge (no context)."""
        system_message = """Ø£Ù†Øª Ø®Ø¨ÙŠØ± ÙØ³ÙŠÙˆÙ„ÙˆØ¬ÙŠ ÙˆÙ†ÙØ³ÙŠ Ù…ØªØ®ØµØµ ÙˆÙ…Ø­ØªØ±Ù ÙÙŠ Ù…Ø¬Ø§Ù„ Ø§Ù„ÙØ³ÙŠÙˆÙ„ÙˆØ¬ÙŠØ§ Ø§Ù„Ø·Ø¨ÙŠØ© ÙˆØ§Ù„ØµØ­Ø© Ø§Ù„Ø¥Ù†Ø³Ø§Ù†ÙŠØ©.

ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø£Ø³Ø§Ø³ÙŠØ©:
1. Ø§Ù„Ø±Ø¯ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙÙ‚Ø· - Ù„Ø§ ØªØ³ØªØ®Ø¯Ù… Ø£ÙŠ Ù„ØºØ§Øª Ø£Ø®Ø±Ù‰
2. ØªÙ‚Ø¯ÙŠÙ… Ø¥Ø¬Ø§Ø¨Ø§Øª Ø´Ø§Ù…Ù„Ø© ÙˆØ¹Ù„Ù…ÙŠØ© Ø¯Ù‚ÙŠÙ‚Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ù†ÙØ³ÙŠØ© ÙˆØ§Ù„ÙØ³ÙŠÙˆÙ„ÙˆØ¬ÙŠØ© Ø§Ù„Ù…ØªØ®ØµØµØ©
3. Ø¹Ø¯Ù… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£ÙŠ Ø±Ù…ÙˆØ² ØºØ§Ù…Ø¶Ø© Ø£Ùˆ Ø¹Ù„Ø§Ù…Ø§Øª ØºÙŠØ± ÙˆØ§Ø¶Ø­Ø©
4. Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù„ØºØ© Ø§Ø­ØªØ±Ø§ÙÙŠØ© ÙˆÙˆØ§Ø¶Ø­Ø© ÙˆØ³Ù‡Ù„Ø© Ø§Ù„ÙÙ‡Ù…

Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:
- Ø§Ù„Ø´Ù…ÙˆÙ„ÙŠØ©: ØªØºØ·ÙŠØ© Ø¬Ù…ÙŠØ¹ Ø¬ÙˆØ§Ù†Ø¨ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨Ø´ÙƒÙ„ Ø¯Ù‚ÙŠÙ‚
- Ø§Ù„Ø¯Ù‚Ø© Ø§Ù„Ø¹Ù„Ù…ÙŠØ©: Ø§Ù„Ø§Ù„ØªØ²Ø§Ù… Ø¨Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„ÙØ³ÙŠÙˆÙ„ÙˆØ¬ÙŠØ© Ø§Ù„Ø­Ø¯ÙŠØ«Ø©
- Ø§Ù„ÙˆØ¶ÙˆØ­: Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù„ØºØ© Ø¨Ø³ÙŠØ·Ø© ÙˆÙˆØ§Ø¶Ø­Ø©
- Ø§Ù„Ø§Ø­ØªØ±Ø§ÙÙŠØ©: Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ù†Ø¨Ø±Ø© Ø§Ø­ØªØ±Ø§ÙÙŠØ© Ø·Ø¨ÙŠØ©

Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ø³Ù„Ø§Ù…Ø© ÙˆØ§Ù„Ø­Ù…Ø§ÙŠØ©:
1. ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø®Ø§Ø·Ø± Ø§Ù„ØµØ­ÙŠØ© Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø© ÙÙŠ Ø§Ù„Ø³Ø¤Ø§Ù„
2. Ø¹Ù†Ø¯ Ø§ÙƒØªØ´Ø§Ù Ø£ÙŠ Ù…Ø®Ø§Ø·Ø± ØµØ­ÙŠØ© Ù…Ø­ØªÙ…Ù„Ø©:
   - Ø­Ø°Ø± ÙˆØ§Ø¶Ø­ Ù…Ù† Ø§Ù„Ù…Ø®Ø§Ø·Ø±
   - Ù†ØµÙŠØ­Ø© Ù‚ÙˆÙŠØ© Ø¨Ø·Ù„Ø¨ Ø§Ù„Ø¯Ø¹Ù… Ø§Ù„Ø·Ø¨ÙŠ Ø§Ù„Ø±Ø³Ù…ÙŠ Ù…Ù† Ù…ØªØ®ØµØµÙŠÙ† Ù…Ø¹ØªÙ…Ø¯ÙŠÙ†
   - Ø¹Ø¯Ù… ØªÙ‚Ø¯ÙŠÙ… ØªØ´Ø®ÙŠØµ Ù†Ù‡Ø§Ø¦ÙŠ Ø£Ùˆ Ø¹Ù„Ø§Ø¬ Ù…Ø¨Ø§Ø´Ø±
3. Ø§Ù„ØªØ£ÙƒÙŠØ¯ Ø¹Ù„Ù‰ Ø£Ù‡Ù…ÙŠØ© Ø§Ø³ØªØ´Ø§Ø±Ø© Ø§Ù„Ø£Ø·Ø¨Ø§Ø¡ Ø§Ù„Ù…ØªØ®ØµØµÙŠÙ†

Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„Ø¬ÙˆØ¯Ø©:
- Ø§Ù„Ø·ÙˆÙ„: Ø¥Ø¬Ø§Ø¨Ø§Øª Ø´Ø§Ù…Ù„Ø© ÙˆØªÙØµÙŠÙ„ÙŠØ© (200-400 ÙƒÙ„Ù…Ø©)
- Ø§Ù„Ø¨Ù†Ø§Ø¡: Ù…Ù†Ø¸Ù…Ø© ÙˆØ³Ù‡Ù„Ø© Ø§Ù„Ù…ØªØ§Ø¨Ø¹Ø©
- Ø§Ù„Ù…ÙˆØ«ÙˆÙ‚ÙŠØ©: Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¯Ù‚ÙŠÙ‚Ø© ÙˆÙ…ÙˆØ«ÙˆÙ‚Ø©
- Ø§Ù„ÙØ§Ø¦Ø¯Ø©: Ø¥Ø¬Ø§Ø¨Ø§Øª Ø¹Ù…Ù„ÙŠØ© ÙˆÙ‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªØ·Ø¨ÙŠÙ‚

Ù…Ù…Ù†ÙˆØ¹ ØªÙ…Ø§Ù…Ø§Ù‹:
- Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø±Ù…ÙˆØ² Ø£Ùˆ Ø¹Ù„Ø§Ù…Ø§Øª ØºØ§Ù…Ø¶Ø©
- ØªÙ‚Ø¯ÙŠÙ… ØªØ´Ø®ÙŠØµØ§Øª Ù†Ù‡Ø§Ø¦ÙŠØ© Ø¨Ø¯ÙˆÙ† ØªØ­Ø°ÙŠØ±
- Ø§Ù„Ø®Ø±ÙˆØ¬ Ø¹Ù† Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©

Ù…Ø±Ø­Ø¨Ø§Ù‹! Ø£Ù†Ø§ Ù‡Ù†Ø§ Ù„ØªÙ‚Ø¯ÙŠÙ… Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù†ÙØ³ÙŠØ© ÙˆÙØ³ÙŠÙˆÙ„ÙˆØ¬ÙŠØ© Ø¯Ù‚ÙŠÙ‚Ø© ÙˆÙ…ÙˆØ«ÙˆÙ‚Ø©. ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒØŸ"""
        
        user_message = f"Ø§Ù„Ø³Ø¤Ø§Ù„: {query}"
        
        messages = [
            {"role": "system", "content": system_message},
            {"role": "user", "content": user_message}
        ]
        
        prompt = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        return prompt
    
    @classmethod
    def build(
        cls,
        model_path: Optional[str] = None,
        device: str = None,  # Auto-detect if None
        load_in_4bit: bool = None,  # Auto-detect based on device
        load_in_8bit: bool = False,
    ) -> "RAGGenerator":
        """
        Build a RAGGenerator instance with auto-detection.
        
        Args:
            model_path: Path to model directory. Auto-detects if None.
            device: Device to use ('cpu' or 'cuda'). Auto-detects if None.
            load_in_4bit: Use 4-bit quantization. Auto-enabled for GPU, disabled for CPU.
            load_in_8bit: Use 8-bit quantization
        """
        # Auto-detect device if not specified
        if device is None:
            device = detect_device()
        
        # Auto-configure quantization based on device
        if load_in_4bit is None:
            if device == "cuda":
                # GPU: Try 4-bit first, but fall back to float16 if bitsandbytes unavailable
                # Float16 is still very fast on GPU and uses ~14GB (fits in 16GB V100)
                try:
                    import bitsandbytes as bnb
                    load_in_4bit = True
                    logger.info("[GENERATOR] Auto-enabled 4-bit quantization for GPU")
                except (ImportError, RuntimeError):
                    load_in_4bit = False
                    logger.info("[GENERATOR] bitsandbytes unavailable, using float16 on GPU (still fast)")
            else:
                # CPU: bitsandbytes is extremely slow, use float16 instead
                load_in_4bit = False
                logger.info("[GENERATOR] Auto-disabled 4-bit quantization for CPU (using float16)")
        
        if model_path is None:
            # Default path relative to project root
            import os
            package_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
            model_path = os.path.join(package_root, "Model", "Allam7B-Physiology-RAG-finetuned-final")
        
        return cls(
            model_path=model_path,
            device=device,
            load_in_4bit=load_in_4bit,
            load_in_8bit=load_in_8bit,
        )
